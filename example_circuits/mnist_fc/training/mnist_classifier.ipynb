{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "364c3303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "364d51c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5053da8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/code/lib/python3.10/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml('mnist_784', as_frame=False, cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "378de361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c329e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist.data.astype('float32')\n",
    "y = mnist.target.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1371b61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X/=255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b79c6e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8f6ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example(X, y):\n",
    "    \"\"\"Plot the first 5 images and their labels in a row.\"\"\"\n",
    "    for i, (img, y) in enumerate(zip(X[:5].reshape(5, 28, 28), y[:5])):\n",
    "        plt.subplot(151 + i)\n",
    "        plt.imshow(img)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16a3765c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_example(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e7757a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "57209156",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X.shape[1]\n",
    "hidden_dim = 20\n",
    "output_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7ad5710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierModule(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=output_dim,\n",
    "            nonlin=F.relu,\n",
    "            dropout=0.5,\n",
    "    ):\n",
    "        super(ClassifierModule, self).__init__()\n",
    "        \n",
    "        self.hidden_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.nonlin = nonlin\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        hid = self.hidden_layer(X)\n",
    "        hid_relu = self.nonlin(hid)\n",
    "        hid_relu_drop = self.dropout(hid_relu)\n",
    "        out = self.output_layer(hid_relu_drop)\n",
    "        probs = F.softmax(out, dim=-1)\n",
    "        return probs, out, hid_relu, hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2b06de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "db654ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierNet(NeuralNetClassifier):\n",
    "    def get_loss(self, y_pred, y_true, *args, **kwargs):\n",
    "        probs, output, hid_relu, hid = y_pred\n",
    "        return super().get_loss(probs, y_true, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "031988ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ClassifierNet(\n",
    "    ClassifierModule,\n",
    "    max_epochs=50,\n",
    "    lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "01fe8a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2053\u001b[0m       \u001b[32m0.8751\u001b[0m        \u001b[35m0.5236\u001b[0m  0.2986\n",
      "      2        \u001b[36m0.8046\u001b[0m       \u001b[32m0.8956\u001b[0m        \u001b[35m0.4176\u001b[0m  0.2731\n",
      "      3        \u001b[36m0.7305\u001b[0m       \u001b[32m0.8998\u001b[0m        \u001b[35m0.3842\u001b[0m  0.2810\n",
      "      4        \u001b[36m0.7069\u001b[0m       \u001b[32m0.9024\u001b[0m        \u001b[35m0.3651\u001b[0m  0.3173\n",
      "      5        \u001b[36m0.6890\u001b[0m       \u001b[32m0.9059\u001b[0m        \u001b[35m0.3490\u001b[0m  0.2811\n",
      "      6        \u001b[36m0.6696\u001b[0m       \u001b[32m0.9094\u001b[0m        \u001b[35m0.3299\u001b[0m  0.2889\n",
      "      7        \u001b[36m0.6669\u001b[0m       \u001b[32m0.9103\u001b[0m        \u001b[35m0.3258\u001b[0m  0.2861\n",
      "      8        \u001b[36m0.6573\u001b[0m       \u001b[32m0.9130\u001b[0m        \u001b[35m0.3156\u001b[0m  0.2743\n",
      "      9        \u001b[36m0.6452\u001b[0m       0.9112        \u001b[35m0.3126\u001b[0m  0.2721\n",
      "     10        \u001b[36m0.6439\u001b[0m       \u001b[32m0.9134\u001b[0m        \u001b[35m0.3075\u001b[0m  0.2715\n",
      "     11        \u001b[36m0.6310\u001b[0m       \u001b[32m0.9156\u001b[0m        \u001b[35m0.3037\u001b[0m  0.2766\n",
      "     12        \u001b[36m0.6304\u001b[0m       \u001b[32m0.9162\u001b[0m        \u001b[35m0.2979\u001b[0m  0.2766\n",
      "     13        \u001b[36m0.6189\u001b[0m       0.9155        \u001b[35m0.2935\u001b[0m  0.2762\n",
      "     14        0.6199       \u001b[32m0.9169\u001b[0m        \u001b[35m0.2918\u001b[0m  0.2727\n",
      "     15        \u001b[36m0.6174\u001b[0m       \u001b[32m0.9206\u001b[0m        \u001b[35m0.2869\u001b[0m  0.2783\n",
      "     16        \u001b[36m0.6121\u001b[0m       0.9206        \u001b[35m0.2836\u001b[0m  0.2736\n",
      "     17        \u001b[36m0.6057\u001b[0m       0.9196        0.2848  0.2774\n",
      "     18        \u001b[36m0.6007\u001b[0m       \u001b[32m0.9221\u001b[0m        \u001b[35m0.2772\u001b[0m  0.2709\n",
      "     19        0.6021       0.9213        0.2773  0.2747\n",
      "     20        0.6098       \u001b[32m0.9231\u001b[0m        \u001b[35m0.2750\u001b[0m  0.2773\n",
      "     21        \u001b[36m0.5904\u001b[0m       \u001b[32m0.9238\u001b[0m        \u001b[35m0.2735\u001b[0m  0.2815\n",
      "     22        0.5916       \u001b[32m0.9253\u001b[0m        \u001b[35m0.2723\u001b[0m  0.2768\n",
      "     23        0.5935       \u001b[32m0.9256\u001b[0m        \u001b[35m0.2663\u001b[0m  0.2778\n",
      "     24        \u001b[36m0.5873\u001b[0m       0.9232        0.2751  0.2799\n",
      "     25        0.5896       0.9246        0.2723  0.2728\n",
      "     26        \u001b[36m0.5872\u001b[0m       0.9253        0.2688  0.2757\n",
      "     27        \u001b[36m0.5832\u001b[0m       \u001b[32m0.9261\u001b[0m        0.2672  0.2743\n",
      "     28        \u001b[36m0.5820\u001b[0m       \u001b[32m0.9281\u001b[0m        \u001b[35m0.2628\u001b[0m  0.2802\n",
      "     29        \u001b[36m0.5761\u001b[0m       0.9272        \u001b[35m0.2625\u001b[0m  0.2843\n",
      "     30        0.5788       0.9275        0.2647  0.2904\n",
      "     31        \u001b[36m0.5754\u001b[0m       \u001b[32m0.9301\u001b[0m        \u001b[35m0.2566\u001b[0m  0.2808\n",
      "     32        0.5782       0.9296        0.2620  0.2734\n",
      "     33        0.5794       \u001b[32m0.9315\u001b[0m        0.2586  0.2918\n",
      "     34        \u001b[36m0.5754\u001b[0m       0.9304        0.2576  0.2783\n",
      "     35        \u001b[36m0.5703\u001b[0m       0.9287        0.2631  0.2806\n",
      "     36        \u001b[36m0.5658\u001b[0m       0.9291        0.2585  0.2826\n",
      "     37        0.5679       0.9307        0.2569  0.2813\n",
      "     38        0.5664       0.9294        0.2573  0.2799\n",
      "     39        \u001b[36m0.5633\u001b[0m       0.9303        \u001b[35m0.2548\u001b[0m  0.2755\n",
      "     40        0.5641       0.9313        0.2596  0.2736\n",
      "     41        0.5636       0.9303        0.2548  0.2746\n",
      "     42        \u001b[36m0.5569\u001b[0m       0.9306        0.2566  0.2779\n",
      "     43        0.5592       \u001b[32m0.9326\u001b[0m        0.2552  0.2770\n",
      "     44        0.5576       0.9306        0.2583  0.2795\n",
      "     45        \u001b[36m0.5527\u001b[0m       0.9323        0.2570  0.2798\n",
      "     46        0.5598       \u001b[32m0.9328\u001b[0m        0.2565  0.2797\n",
      "     47        0.5572       0.9328        \u001b[35m0.2514\u001b[0m  0.2783\n",
      "     48        0.5560       \u001b[32m0.9347\u001b[0m        0.2527  0.2838\n",
      "     49        0.5630       0.9320        0.2547  0.2785\n",
      "     50        0.5544       0.9314        0.2547  0.2750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class '__main__.ClassifierNet'>[initialized](\n",
       "  module_=ClassifierModule(\n",
       "    (hidden_layer): Linear(in_features=784, out_features=20, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (output_layer): Linear(in_features=20, out_features=10, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6b8a5327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 4, 5, 7, 7])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making prediction for first 5 data points of X\n",
    "y_pred = net.predict(X_test[:5])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f85889a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 4, 5, 7, 7])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_out = net.forward(X_test[:5])\n",
    "y_pred = y_out[1].numpy().argmax(1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a38f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking probarbility of each class for first 5 data points of X\n",
    "y_proba = net.predict_proba(X[:5])\n",
    "y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2f0cf8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9313571428571429"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = net.predict(X_test)\n",
    "sum(y_test == y_pred)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735718a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(net.get_all_learnable_params())['hidden_layer.weight'].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9287e392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_layer.weight': array([-0.02448081,  0.03203928, -0.00945938, ..., -0.00975679,\n",
       "         0.00860115,  0.01197949], dtype=float32),\n",
       " 'hidden_layer.bias': array([ 0.11924172, -0.25702068, -0.3657261 , -0.5371215 , -0.49257886,\n",
       "        -0.6122999 , -0.6142976 ,  1.1997513 ,  0.1511106 ,  0.05111085,\n",
       "        -0.19184524,  0.24291962, -1.0575203 , -0.37877414,  0.10841223,\n",
       "         0.21820208,  0.9989543 ,  0.53919125,  0.37816167,  0.16991903],\n",
       "       dtype=float32),\n",
       " 'output_layer.weight': array([-0.41038993, -0.8864554 , -0.95346326, -1.1938698 , -0.96469045,\n",
       "         0.65605104, -1.4380188 ,  0.73516756,  0.01890494,  0.6669712 ,\n",
       "         0.5050056 , -0.87012583,  0.7344386 , -0.7996258 ,  0.61766964,\n",
       "         0.63174284,  0.81355214, -1.137027  , -0.32367334, -1.0747054 ,\n",
       "        -1.2947724 ,  0.6419555 , -0.7560812 , -0.64076567,  0.8293333 ,\n",
       "        -0.9234198 ,  0.7114406 , -1.2430638 , -0.5064362 , -0.8474891 ,\n",
       "        -0.32151207, -0.999014  , -1.173742  ,  0.73906523, -1.2967974 ,\n",
       "        -0.58627635,  0.5783761 ,  1.0636133 , -0.78371406,  0.8398031 ,\n",
       "         0.6946079 ,  0.7957569 , -1.0013963 ,  0.43080503,  0.903613  ,\n",
       "         0.49836817, -0.2536826 , -0.26813295,  1.2825528 ,  0.09097729,\n",
       "         0.5178733 , -0.23260093,  0.65064394,  0.7665086 ,  0.10283384,\n",
       "         0.5851539 ,  0.82613957, -0.11172841,  0.74921834, -0.15629657,\n",
       "        -0.28102252,  0.77236557,  1.521384  ,  0.1916936 ,  0.52208734,\n",
       "         0.62220407,  0.44099247, -0.7818951 ,  1.1934701 , -0.42732215,\n",
       "         0.47099155, -0.21878853,  0.28956547,  0.22575709, -0.37351704,\n",
       "         0.5800604 , -0.02847239,  1.3795224 ,  0.1635345 , -0.73645204,\n",
       "         0.7040897 , -1.1320469 , -0.00310648,  0.5961793 , -1.1113659 ,\n",
       "        -1.1767821 ,  0.6533724 ,  0.5853257 , -1.0032666 ,  0.2605468 ,\n",
       "        -0.7040254 ,  0.54322785, -0.91572696, -0.39648435,  0.58784056,\n",
       "        -1.424094  , -0.07746486, -0.38110554,  0.696567  , -0.51846546,\n",
       "        -0.01360776,  0.6028674 ,  1.5138152 ,  0.36670223, -0.22004169,\n",
       "         0.63311076, -0.31287453,  0.5279667 , -0.08069857,  0.55042285,\n",
       "         0.44306967,  0.5017102 ,  0.10417835,  0.17684408,  0.61380476,\n",
       "         0.50716823,  0.26182973,  1.3428687 ,  0.3729626 ,  0.8745667 ,\n",
       "         0.7298387 , -0.46589664, -0.16632387, -0.16614181, -0.8857892 ,\n",
       "        -0.2556697 , -1.0582634 , -0.23556675, -1.1263652 ,  0.5831239 ,\n",
       "         0.5154267 , -1.2504271 , -0.03832624,  0.75399435,  0.6550423 ,\n",
       "        -1.131182  ,  0.84103245, -1.0074184 ,  0.744571  ,  0.9047652 ,\n",
       "        -0.20916368, -0.6884282 , -0.79055655, -0.8174309 ,  0.89869857,\n",
       "        -0.7528036 , -0.10318321,  0.72679627,  1.1102065 , -1.0001224 ,\n",
       "        -1.4440416 ,  0.59378964, -0.19946468, -0.11904068, -1.156105  ,\n",
       "         0.62765664, -1.1599551 , -0.938825  , -0.79918677, -1.0629636 ,\n",
       "        -0.44784024,  0.654701  ,  0.08391974,  0.54883933,  0.82086056,\n",
       "         0.5814452 ,  0.56817657, -0.5795763 , -0.0171244 ,  0.50514543,\n",
       "         0.41184068,  0.52850443,  0.56505007,  0.65936965,  0.5059879 ,\n",
       "         0.38001776, -0.3689377 , -0.34173617, -0.39339545,  0.59682566,\n",
       "         0.62783533, -0.81007844,  1.2168678 ,  0.49947113, -0.21387249,\n",
       "         0.06314833,  0.67301315, -0.31455448, -0.83100736, -0.60750014,\n",
       "        -0.70967287,  0.5732943 ,  0.45543396, -1.2920324 , -0.34652993,\n",
       "         0.4274494 , -1.1506833 ,  0.3698626 , -0.8488577 , -0.43552968],\n",
       "       dtype=float32),\n",
       " 'output_layer.bias': array([-0.51202846,  1.2586364 , -1.7698207 , -0.45534086,  0.58884686,\n",
       "        -1.1442558 , -0.40298715,  1.7893413 , -0.63138884,  1.350203  ],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data = {}\n",
    "\n",
    "for param in net.get_all_learnable_params():\n",
    "    model_data[param[0]] = param[1].detach().numpy().flatten()\n",
    "    \n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9afdb10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c9e540b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,  113725,  772549, 1000000,\n",
       "        545098,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,  333333,  984314,  992157,\n",
       "        984314,  662745,  219608,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,  223529,  552941,  992157,  996078,\n",
       "        992157,  996078,  992157,  996078,  545098,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,  658824,  992157,  984314,  992157,\n",
       "        984314,  992157,  984314,  992157,  984314,  662745,  219608,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,  662745, 1000000,  992157,  447059,\n",
       "        447059,  776471,  992157,  996078,  992157,  996078,  545098,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,  658824,  992157,  984314,       0,\n",
       "             0,  109804,  768627,  992157,  984314,  992157,  984314,\n",
       "        447059,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,  552941,  992157, 1000000,  329412,       0,\n",
       "             0,       0,       0,  223529,  882353,  996078,  992157,\n",
       "        996078,  329412,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,  992157,  984314,  768627,  109804,       0,\n",
       "             0,       0,       0,       0,  658824,  992157,  984314,\n",
       "        992157,  768627,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,  662745,  996078,  992157,  662745,       0,       0,\n",
       "             0,       0,       0,       0,       0,  996078,  992157,\n",
       "        996078,  992157,  223529,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,  658824,  992157,  984314,  219608,       0,       0,\n",
       "             0,       0,       0,       0,       0,  545098,  984314,\n",
       "        992157,  984314,  658824,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,  223529,\n",
       "        552941,  992157,  996078,  768627,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,  333333,  992157,\n",
       "        996078,  992157,  662745,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,  658824,\n",
       "        992157,  984314,  992157,  325490,       0,       0,       0,\n",
       "             0,       0,       0,       0,  447059,  772549,  984314,\n",
       "        992157,  984314,  658824,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,  662745,\n",
       "        996078,  992157,  447059,       0,       0,       0,       0,\n",
       "             0,       0,  223529,  333333,  772549,  996078,  992157,\n",
       "        996078,  992157,  447059,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,  223529,  878431,\n",
       "        992157,  984314,       0,       0,       0,       0,       0,\n",
       "             0,  223529,  878431,  992157,  984314,  992157,  984314,\n",
       "        992157,  541176,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,  337255,  992157,\n",
       "        996078,  992157,  223529,       0,  113725,  772549,  996078,\n",
       "        992157,  996078,  992157,  996078,  992157,  996078,  992157,\n",
       "        996078,  329412,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,  333333,  984314,\n",
       "        992157,  984314,  882353,  658824,  772549,  984314,  992157,\n",
       "        984314,  992157,  984314,  992157,  984314,  992157,  984314,\n",
       "        329412,  109804,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,  337255,  992157,\n",
       "        996078,  992157,  996078,  992157,  996078,  992157,  996078,\n",
       "        992157,  996078,  992157,  996078,  992157,  662745,  219608,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,  333333,  984314,\n",
       "        992157,  984314,  992157,  984314,  992157,  984314,  992157,\n",
       "        984314,  992157,  984314,  768627,  325490,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,  223529,  882353,\n",
       "        996078,  992157,  996078,  992157,  996078,  992157,  662745,\n",
       "        658824,  447059,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,  219608,\n",
       "        545098,  984314,  992157,  541176,  329412,  325490,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0,\n",
       "             0,       0,       0,       0,       0,       0,       0])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_test[idx]*(10**6)).round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0bb12940",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x128 and 784x20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m y\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/code/lib/python3.10/site-packages/skorch/net.py:1469\u001b[0m, in \u001b[0;36mNeuralNet.forward\u001b[0;34m(self, X, training, device)\u001b[0m\n\u001b[1;32m   1427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Gather and concatenate the output from forward call with\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;124;03m    input data.\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1467\u001b[0m \n\u001b[1;32m   1468\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1469\u001b[0m     y_infer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1471\u001b[0m     is_multioutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_infer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y_infer[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mtuple\u001b[39m)\n\u001b[1;32m   1472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_multioutput:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/code/lib/python3.10/site-packages/skorch/net.py:1424\u001b[0m, in \u001b[0;36mNeuralNet.forward_iter\u001b[0;34m(self, X, training, device)\u001b[0m\n\u001b[1;32m   1422\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(dataset, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[0;32m-> 1424\u001b[0m     yp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1425\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m to_device(yp, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/code/lib/python3.10/site-packages/skorch/net.py:1117\u001b[0m, in \u001b[0;36mNeuralNet.evaluation_step\u001b[0;34m(self, batch, training)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(training):\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_training(training)\n\u001b[0;32m-> 1117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/code/lib/python3.10/site-packages/skorch/net.py:1504\u001b[0m, in \u001b[0;36mNeuralNet.infer\u001b[0;34m(self, x, **fit_params)\u001b[0m\n\u001b[1;32m   1502\u001b[0m     x_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_x_and_fit_params(x, fit_params)\n\u001b[1;32m   1503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mx_dict)\n\u001b[0;32m-> 1504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/code/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[77], line 18\u001b[0m, in \u001b[0;36mClassifierModule.forward\u001b[0;34m(self, X, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 18\u001b[0m     hid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     hid_relu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnonlin(hid)\n\u001b[1;32m     20\u001b[0m     hid_relu_drop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hid_relu)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/code/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/code/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x128 and 784x20)"
     ]
    }
   ],
   "source": [
    "y = net.forward(X_test[idx])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "93126503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3.2616e-13, 2.8814e-12, 5.8566e-04, 1.6586e-05, 6.3747e-14, 9.8997e-04,\n",
       "          3.3239e-12, 3.1290e-14, 9.9841e-01, 3.0843e-10],\n",
       "         [4.5642e-08, 5.3214e-09, 2.4170e-05, 6.7821e-06, 9.7625e-01, 2.0017e-03,\n",
       "          2.2955e-07, 5.5003e-04, 9.8854e-04, 2.0178e-02],\n",
       "         [2.1775e-02, 2.6817e-04, 5.7213e-03, 1.7085e-03, 8.0426e-03, 4.4847e-01,\n",
       "          1.6200e-01, 1.6889e-05, 3.5016e-01, 1.8330e-03],\n",
       "         [1.2655e-03, 9.7786e-06, 6.2008e-02, 1.4975e-01, 3.2650e-09, 8.7413e-04,\n",
       "          6.0122e-09, 7.8340e-01, 2.1671e-03, 5.2160e-04],\n",
       "         [8.6026e-04, 9.0423e-06, 1.1639e-03, 1.1740e-03, 3.1760e-06, 4.8511e-03,\n",
       "          6.9705e-08, 9.7680e-01, 2.9877e-03, 1.2154e-02]]),\n",
       " tensor([[-10.0919,  -7.9132,  11.2167,   7.6526, -11.7243,  11.7417,  -7.7704,\n",
       "          -12.4360,  18.6579,  -3.2400],\n",
       "         [ -7.6701,  -9.8192,  -1.3981,  -2.6689,   9.2083,   3.0186,  -6.0548,\n",
       "            1.7268,   2.3130,   5.3291],\n",
       "         [  0.8233,  -3.5736,  -0.5133,  -1.7219,  -0.1728,   3.8483,   2.8301,\n",
       "           -6.3386,   3.6009,  -1.6515],\n",
       "         [  1.9170,  -2.9461,   5.8088,   6.6905, -10.9508,   1.5470, -10.3402,\n",
       "            8.3452,   2.4549,   1.0307],\n",
       "         [  0.6841,  -3.8712,   0.9865,   0.9951,  -4.9175,   2.4139,  -8.7366,\n",
       "            7.7189,   1.9291,   3.3323]]),\n",
       " tensor([[0.0000, 3.7986, 0.0000, 3.3534, 3.4942, 4.8359, 1.9627, 0.0000, 0.0000,\n",
       "          3.0290, 2.4979, 3.2614, 1.1434, 1.5907, 2.1750, 1.0767, 0.0000, 0.0000,\n",
       "          0.0000, 1.1385],\n",
       "         [1.9452, 0.0000, 0.0000, 0.7569, 0.0000, 0.0000, 2.4357, 1.7195, 0.0000,\n",
       "          0.0000, 0.0000, 4.6428, 0.0000, 0.0000, 1.7985, 0.0000, 0.0000, 0.0000,\n",
       "          0.8921, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6289, 0.0000, 0.0000, 0.0000,\n",
       "          1.2244, 1.9781, 0.8667, 0.0000, 0.0000, 2.2360, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 1.4141],\n",
       "         [0.0000, 0.0000, 0.0764, 0.0000, 0.9595, 0.2123, 0.0000, 0.0000, 2.7610,\n",
       "          0.0000, 0.3533, 0.2199, 0.0000, 0.0000, 0.0000, 5.1429, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 1.2403, 0.0000, 0.0000, 1.6241, 0.0000,\n",
       "          0.0000, 0.0000, 1.6757, 0.0000, 0.0000, 0.0000, 4.2054, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]]),\n",
       " tensor([[-5.3248,  3.7986, -1.5443,  3.3534,  3.4942,  4.8359,  1.9627, -1.2116,\n",
       "          -1.3402,  3.0290,  2.4979,  3.2614,  1.1434,  1.5907,  2.1750,  1.0767,\n",
       "          -3.6957, -1.7340, -4.6527,  1.1385],\n",
       "         [ 1.9452, -3.5949, -0.9688,  0.7569, -3.1692, -1.4893,  2.4357,  1.7195,\n",
       "          -1.3249, -1.2912, -2.1978,  4.6428, -3.1335, -2.4919,  1.7985, -1.8011,\n",
       "          -3.2939, -0.8385,  0.8921, -1.9009],\n",
       "         [-2.2116, -1.2139, -1.1658, -0.2041, -0.6338,  0.6289, -2.3349, -1.0753,\n",
       "          -2.9673,  1.2244,  1.9781,  0.8667, -3.2417, -0.0151,  2.2360, -1.0155,\n",
       "          -1.8531, -1.5627, -2.9465,  1.4141],\n",
       "         [-1.2523, -0.7207,  0.0764, -1.5482,  0.9595,  0.2123, -1.3547, -0.2739,\n",
       "           2.7610, -2.2330,  0.3533,  0.2199, -0.7317, -2.0989, -2.6424,  5.1429,\n",
       "          -1.4586, -0.6894, -2.0487, -2.8618],\n",
       "         [-1.7375, -1.6941, -0.8169, -1.2670,  1.2403, -0.5202, -0.4197,  1.6241,\n",
       "          -1.7084, -1.7899, -1.1269,  1.6757, -0.8283, -3.0806, -2.3460,  4.2054,\n",
       "          -2.4037, -0.6209, -3.7515, -2.8111]]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = X_test[:5]\n",
    "y = net.forward(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c4ea94ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3.2616e-13, 2.8814e-12, 5.8566e-04, 1.6586e-05, 6.3747e-14, 9.8997e-04,\n",
       "          3.3239e-12, 3.1290e-14, 9.9841e-01, 3.0843e-10],\n",
       "         [4.5642e-08, 5.3214e-09, 2.4170e-05, 6.7821e-06, 9.7625e-01, 2.0017e-03,\n",
       "          2.2955e-07, 5.5003e-04, 9.8854e-04, 2.0178e-02],\n",
       "         [2.1775e-02, 2.6817e-04, 5.7213e-03, 1.7085e-03, 8.0426e-03, 4.4847e-01,\n",
       "          1.6200e-01, 1.6889e-05, 3.5016e-01, 1.8330e-03],\n",
       "         [1.2655e-03, 9.7786e-06, 6.2008e-02, 1.4975e-01, 3.2650e-09, 8.7413e-04,\n",
       "          6.0122e-09, 7.8340e-01, 2.1671e-03, 5.2160e-04],\n",
       "         [8.6026e-04, 9.0423e-06, 1.1639e-03, 1.1740e-03, 3.1760e-06, 4.8511e-03,\n",
       "          6.9705e-08, 9.7680e-01, 2.9877e-03, 1.2154e-02]]),\n",
       " tensor([[-10.0919,  -7.9132,  11.2167,   7.6526, -11.7243,  11.7417,  -7.7704,\n",
       "          -12.4360,  18.6579,  -3.2400],\n",
       "         [ -7.6701,  -9.8192,  -1.3981,  -2.6689,   9.2083,   3.0186,  -6.0548,\n",
       "            1.7268,   2.3130,   5.3291],\n",
       "         [  0.8233,  -3.5736,  -0.5133,  -1.7219,  -0.1728,   3.8483,   2.8301,\n",
       "           -6.3386,   3.6009,  -1.6515],\n",
       "         [  1.9170,  -2.9461,   5.8088,   6.6905, -10.9508,   1.5470, -10.3402,\n",
       "            8.3452,   2.4549,   1.0307],\n",
       "         [  0.6841,  -3.8712,   0.9865,   0.9951,  -4.9175,   2.4139,  -8.7366,\n",
       "            7.7189,   1.9291,   3.3323]]),\n",
       " tensor([[0.0000, 3.7986, 0.0000, 3.3534, 3.4942, 4.8359, 1.9627, 0.0000, 0.0000,\n",
       "          3.0290, 2.4979, 3.2614, 1.1434, 1.5907, 2.1750, 1.0767, 0.0000, 0.0000,\n",
       "          0.0000, 1.1385],\n",
       "         [1.9452, 0.0000, 0.0000, 0.7569, 0.0000, 0.0000, 2.4357, 1.7195, 0.0000,\n",
       "          0.0000, 0.0000, 4.6428, 0.0000, 0.0000, 1.7985, 0.0000, 0.0000, 0.0000,\n",
       "          0.8921, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6289, 0.0000, 0.0000, 0.0000,\n",
       "          1.2244, 1.9781, 0.8667, 0.0000, 0.0000, 2.2360, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 1.4141],\n",
       "         [0.0000, 0.0000, 0.0764, 0.0000, 0.9595, 0.2123, 0.0000, 0.0000, 2.7610,\n",
       "          0.0000, 0.3533, 0.2199, 0.0000, 0.0000, 0.0000, 5.1429, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 1.2403, 0.0000, 0.0000, 1.6241, 0.0000,\n",
       "          0.0000, 0.0000, 1.6757, 0.0000, 0.0000, 0.0000, 4.2054, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000]]),\n",
       " tensor([[-5.3248,  3.7986, -1.5443,  3.3534,  3.4942,  4.8359,  1.9627, -1.2116,\n",
       "          -1.3402,  3.0290,  2.4979,  3.2614,  1.1434,  1.5907,  2.1750,  1.0767,\n",
       "          -3.6957, -1.7340, -4.6527,  1.1385],\n",
       "         [ 1.9452, -3.5949, -0.9688,  0.7569, -3.1692, -1.4893,  2.4357,  1.7195,\n",
       "          -1.3249, -1.2912, -2.1978,  4.6428, -3.1335, -2.4919,  1.7985, -1.8011,\n",
       "          -3.2939, -0.8385,  0.8921, -1.9009],\n",
       "         [-2.2116, -1.2139, -1.1658, -0.2041, -0.6338,  0.6289, -2.3349, -1.0753,\n",
       "          -2.9673,  1.2244,  1.9781,  0.8667, -3.2417, -0.0151,  2.2360, -1.0155,\n",
       "          -1.8531, -1.5627, -2.9465,  1.4141],\n",
       "         [-1.2523, -0.7207,  0.0764, -1.5482,  0.9595,  0.2123, -1.3547, -0.2739,\n",
       "           2.7610, -2.2330,  0.3533,  0.2199, -0.7317, -2.0989, -2.6424,  5.1429,\n",
       "          -1.4586, -0.6894, -2.0487, -2.8618],\n",
       "         [-1.7375, -1.6941, -0.8169, -1.2670,  1.2403, -0.5202, -0.4197,  1.6241,\n",
       "          -1.7084, -1.7899, -1.1269,  1.6757, -0.8283, -3.0806, -2.3460,  4.2054,\n",
       "          -2.4037, -0.6209, -3.7515, -2.8111]]))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6efa733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_json = {\n",
    "    \"input\": (x[0]*(10**6)).round().astype(int).tolist(),\n",
    "    \"hid_weights\": (model_data['hidden_layer.weight']*(10**6)).round().astype(int).tolist(),\n",
    "    \"hid_bias\": (model_data['hidden_layer.bias']*(10**12)).round().astype(int).tolist(),\n",
    "    \"out_weights\":(model_data['output_layer.weight']*(10**6)).round().astype(int).tolist(),\n",
    "    \"out_bias\": (model_data['output_layer.bias']*(10**18)).round().astype(int).tolist() # can also change to zero because we are doing argmax instead of softmax\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d9336d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  376471,\n",
       "  474510,\n",
       "  474510,\n",
       "  474510,\n",
       "  474510,\n",
       "  745098,\n",
       "  474510,\n",
       "  745098,\n",
       "  745098,\n",
       "  635294,\n",
       "  854902,\n",
       "  474510,\n",
       "  474510,\n",
       "  423529,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  180392,\n",
       "  419608,\n",
       "  858824,\n",
       "  976471,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  992157,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  984314,\n",
       "  941176,\n",
       "  941176,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  176471,\n",
       "  905882,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  909804,\n",
       "  619608,\n",
       "  854902,\n",
       "  988235,\n",
       "  792157,\n",
       "  992157,\n",
       "  988235,\n",
       "  850980,\n",
       "  929412,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  752941,\n",
       "  592157,\n",
       "  101961,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  913725,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  764706,\n",
       "  121569,\n",
       "  0,\n",
       "  98039,\n",
       "  152941,\n",
       "  74510,\n",
       "  152941,\n",
       "  152941,\n",
       "  98039,\n",
       "  396078,\n",
       "  988235,\n",
       "  988235,\n",
       "  415686,\n",
       "  58824,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  729412,\n",
       "  976471,\n",
       "  988235,\n",
       "  988235,\n",
       "  933333,\n",
       "  400000,\n",
       "  82353,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  98039,\n",
       "  768627,\n",
       "  988235,\n",
       "  988235,\n",
       "  309804,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  466667,\n",
       "  854902,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  729412,\n",
       "  188235,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  90196,\n",
       "  501961,\n",
       "  988235,\n",
       "  988235,\n",
       "  831373,\n",
       "  82353,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  117647,\n",
       "  749020,\n",
       "  870588,\n",
       "  988235,\n",
       "  988235,\n",
       "  952941,\n",
       "  650980,\n",
       "  74510,\n",
       "  78431,\n",
       "  784314,\n",
       "  988235,\n",
       "  988235,\n",
       "  945098,\n",
       "  380392,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  98039,\n",
       "  474510,\n",
       "  952941,\n",
       "  988235,\n",
       "  988235,\n",
       "  796078,\n",
       "  800000,\n",
       "  988235,\n",
       "  988235,\n",
       "  952941,\n",
       "  376471,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  286275,\n",
       "  776471,\n",
       "  988235,\n",
       "  988235,\n",
       "  992157,\n",
       "  988235,\n",
       "  968627,\n",
       "  639216,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  360784,\n",
       "  815686,\n",
       "  988235,\n",
       "  988235,\n",
       "  992157,\n",
       "  988235,\n",
       "  619608,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  125490,\n",
       "  474510,\n",
       "  827451,\n",
       "  992157,\n",
       "  992157,\n",
       "  992157,\n",
       "  1000000,\n",
       "  992157,\n",
       "  800000,\n",
       "  74510,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  133333,\n",
       "  568627,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  764706,\n",
       "  772549,\n",
       "  988235,\n",
       "  988235,\n",
       "  780392,\n",
       "  90196,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  137255,\n",
       "  839216,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  949020,\n",
       "  592157,\n",
       "  47059,\n",
       "  50980,\n",
       "  945098,\n",
       "  988235,\n",
       "  988235,\n",
       "  207843,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  356863,\n",
       "  890196,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  772549,\n",
       "  329412,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  772549,\n",
       "  988235,\n",
       "  988235,\n",
       "  207843,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  831373,\n",
       "  988235,\n",
       "  988235,\n",
       "  917647,\n",
       "  615686,\n",
       "  66667,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  556863,\n",
       "  988235,\n",
       "  988235,\n",
       "  207843,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  101961,\n",
       "  949020,\n",
       "  988235,\n",
       "  988235,\n",
       "  615686,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  211765,\n",
       "  949020,\n",
       "  988235,\n",
       "  988235,\n",
       "  207843,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  474510,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  207843,\n",
       "  0,\n",
       "  109804,\n",
       "  160784,\n",
       "  160784,\n",
       "  435294,\n",
       "  992157,\n",
       "  988235,\n",
       "  988235,\n",
       "  690196,\n",
       "  66667,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  172549,\n",
       "  956863,\n",
       "  988235,\n",
       "  988235,\n",
       "  701961,\n",
       "  627451,\n",
       "  874510,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  992157,\n",
       "  988235,\n",
       "  698039,\n",
       "  58824,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  635294,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  941176,\n",
       "  439216,\n",
       "  43137,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  200000,\n",
       "  658824,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  988235,\n",
       "  682353,\n",
       "  466667,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'hid_weights': [-24481,\n",
       "  32039,\n",
       "  -9459,\n",
       "  9798,\n",
       "  5743,\n",
       "  -10065,\n",
       "  -5840,\n",
       "  -29256,\n",
       "  6858,\n",
       "  -15276,\n",
       "  18512,\n",
       "  28399,\n",
       "  18262,\n",
       "  20336,\n",
       "  10834,\n",
       "  17110,\n",
       "  33219,\n",
       "  4492,\n",
       "  30956,\n",
       "  -7038,\n",
       "  30536,\n",
       "  12509,\n",
       "  -26958,\n",
       "  -14882,\n",
       "  -25450,\n",
       "  -17009,\n",
       "  -1431,\n",
       "  34198,\n",
       "  25756,\n",
       "  -15040,\n",
       "  -3103,\n",
       "  11548,\n",
       "  28405,\n",
       "  12538,\n",
       "  -35561,\n",
       "  18232,\n",
       "  14822,\n",
       "  -27944,\n",
       "  44120,\n",
       "  37548,\n",
       "  -13419,\n",
       "  12707,\n",
       "  -40680,\n",
       "  -48642,\n",
       "  -46158,\n",
       "  -25871,\n",
       "  42151,\n",
       "  -24709,\n",
       "  -7366,\n",
       "  -28652,\n",
       "  35930,\n",
       "  10797,\n",
       "  -6562,\n",
       "  21610,\n",
       "  -9583,\n",
       "  -21489,\n",
       "  -7455,\n",
       "  22021,\n",
       "  -8118,\n",
       "  -30602,\n",
       "  -10318,\n",
       "  -24736,\n",
       "  11068,\n",
       "  29136,\n",
       "  16476,\n",
       "  85175,\n",
       "  83428,\n",
       "  88800,\n",
       "  64756,\n",
       "  2421,\n",
       "  87467,\n",
       "  68335,\n",
       "  53577,\n",
       "  76270,\n",
       "  25335,\n",
       "  98879,\n",
       "  76491,\n",
       "  66764,\n",
       "  42201,\n",
       "  -13381,\n",
       "  1970,\n",
       "  -26212,\n",
       "  -19111,\n",
       "  -34496,\n",
       "  29687,\n",
       "  6568,\n",
       "  -35023,\n",
       "  -7656,\n",
       "  -4952,\n",
       "  -4171,\n",
       "  14351,\n",
       "  47641,\n",
       "  99164,\n",
       "  155493,\n",
       "  155849,\n",
       "  212750,\n",
       "  62472,\n",
       "  34559,\n",
       "  159600,\n",
       "  185563,\n",
       "  63041,\n",
       "  171609,\n",
       "  100428,\n",
       "  178518,\n",
       "  194295,\n",
       "  216487,\n",
       "  143305,\n",
       "  28731,\n",
       "  5732,\n",
       "  -19274,\n",
       "  -31324,\n",
       "  13614,\n",
       "  4959,\n",
       "  -25813,\n",
       "  -14359,\n",
       "  29213,\n",
       "  35573,\n",
       "  1197,\n",
       "  17213,\n",
       "  55166,\n",
       "  134855,\n",
       "  151423,\n",
       "  164065,\n",
       "  124411,\n",
       "  74267,\n",
       "  75995,\n",
       "  66905,\n",
       "  47007,\n",
       "  41371,\n",
       "  90424,\n",
       "  46716,\n",
       "  76929,\n",
       "  126869,\n",
       "  116960,\n",
       "  61395,\n",
       "  65995,\n",
       "  -48391,\n",
       "  -17311,\n",
       "  -12031,\n",
       "  -25289,\n",
       "  13919,\n",
       "  -35217,\n",
       "  -16381,\n",
       "  -14424,\n",
       "  4577,\n",
       "  -29625,\n",
       "  -45220,\n",
       "  -4148,\n",
       "  129459,\n",
       "  125515,\n",
       "  92740,\n",
       "  19167,\n",
       "  -62599,\n",
       "  -145781,\n",
       "  -59053,\n",
       "  -58568,\n",
       "  -186850,\n",
       "  -102689,\n",
       "  -126241,\n",
       "  -58320,\n",
       "  4434,\n",
       "  -21196,\n",
       "  29900,\n",
       "  60173,\n",
       "  2170,\n",
       "  -12465,\n",
       "  10761,\n",
       "  4094,\n",
       "  20629,\n",
       "  -31803,\n",
       "  -5954,\n",
       "  2343,\n",
       "  22936,\n",
       "  -57800,\n",
       "  -36916,\n",
       "  32995,\n",
       "  23352,\n",
       "  39308,\n",
       "  -138642,\n",
       "  -81597,\n",
       "  -20012,\n",
       "  -73703,\n",
       "  -38142,\n",
       "  27055,\n",
       "  -59845,\n",
       "  118244,\n",
       "  -247,\n",
       "  -120735,\n",
       "  -149622,\n",
       "  -74264,\n",
       "  -96147,\n",
       "  -78959,\n",
       "  -57435,\n",
       "  -114109,\n",
       "  -44263,\n",
       "  -25206,\n",
       "  30501,\n",
       "  -11085,\n",
       "  14139,\n",
       "  -9520,\n",
       "  20734,\n",
       "  -5685,\n",
       "  -88563,\n",
       "  -85051,\n",
       "  -178390,\n",
       "  -195683,\n",
       "  -112384,\n",
       "  -92966,\n",
       "  -58095,\n",
       "  -15382,\n",
       "  -83070,\n",
       "  84662,\n",
       "  -59639,\n",
       "  83458,\n",
       "  -96359,\n",
       "  -33909,\n",
       "  -40025,\n",
       "  -44811,\n",
       "  -56329,\n",
       "  -78362,\n",
       "  -121574,\n",
       "  -121931,\n",
       "  -54889,\n",
       "  -10590,\n",
       "  5452,\n",
       "  -7996,\n",
       "  -6201,\n",
       "  -10368,\n",
       "  -31592,\n",
       "  -66624,\n",
       "  -94072,\n",
       "  -108236,\n",
       "  -194476,\n",
       "  -43208,\n",
       "  -75779,\n",
       "  -168293,\n",
       "  -49277,\n",
       "  -69032,\n",
       "  -80471,\n",
       "  -57564,\n",
       "  -160348,\n",
       "  -166294,\n",
       "  -137890,\n",
       "  -139943,\n",
       "  -65246,\n",
       "  -62109,\n",
       "  -109717,\n",
       "  -176345,\n",
       "  -178015,\n",
       "  -154819,\n",
       "  -33945,\n",
       "  -23215,\n",
       "  21105,\n",
       "  -5030,\n",
       "  -29184,\n",
       "  -29568,\n",
       "  -69159,\n",
       "  -90590,\n",
       "  -57263,\n",
       "  -61451,\n",
       "  -61310,\n",
       "  -4654,\n",
       "  2869,\n",
       "  -106323,\n",
       "  -58900,\n",
       "  -135956,\n",
       "  -5903,\n",
       "  -83913,\n",
       "  -98500,\n",
       "  -181974,\n",
       "  -90109,\n",
       "  -132452,\n",
       "  -79673,\n",
       "  -106392,\n",
       "  -180391,\n",
       "  -237688,\n",
       "  -299894,\n",
       "  -203377,\n",
       "  -82286,\n",
       "  -16359,\n",
       "  -30546,\n",
       "  16366,\n",
       "  -914,\n",
       "  -108753,\n",
       "  -129615,\n",
       "  -113086,\n",
       "  -51954,\n",
       "  5790,\n",
       "  -6867,\n",
       "  -65293,\n",
       "  34612,\n",
       "  -64831,\n",
       "  -30535,\n",
       "  -32050,\n",
       "  -248270,\n",
       "  -274939,\n",
       "  -197275,\n",
       "  -167354,\n",
       "  -76358,\n",
       "  -87409,\n",
       "  -210203,\n",
       "  -90050,\n",
       "  -159376,\n",
       "  -323712,\n",
       "  -365279,\n",
       "  -172027,\n",
       "  -110222,\n",
       "  -42980,\n",
       "  14055,\n",
       "  -25103,\n",
       "  -31839,\n",
       "  -99837,\n",
       "  -105092,\n",
       "  -95910,\n",
       "  6600,\n",
       "  47865,\n",
       "  106552,\n",
       "  39012,\n",
       "  -46829,\n",
       "  -135019,\n",
       "  -41290,\n",
       "  -283258,\n",
       "  -337494,\n",
       "  -149679,\n",
       "  1998,\n",
       "  39895,\n",
       "  65042,\n",
       "  102224,\n",
       "  47890,\n",
       "  41992,\n",
       "  -108189,\n",
       "  -284557,\n",
       "  -315314,\n",
       "  -163798,\n",
       "  -107943,\n",
       "  -39739,\n",
       "  -18574,\n",
       "  39652,\n",
       "  3932,\n",
       "  -67456,\n",
       "  9833,\n",
       "  -12277,\n",
       "  -875,\n",
       "  -28819,\n",
       "  403,\n",
       "  20213,\n",
       "  105810,\n",
       "  133260,\n",
       "  -43614,\n",
       "  -91583,\n",
       "  -67894,\n",
       "  51218,\n",
       "  160224,\n",
       "  13241,\n",
       "  -10511,\n",
       "  145072,\n",
       "  -30354,\n",
       "  143375,\n",
       "  -40564,\n",
       "  -70906,\n",
       "  -119282,\n",
       "  -111112,\n",
       "  -74351,\n",
       "  7474,\n",
       "  -33635,\n",
       "  29810,\n",
       "  -29939,\n",
       "  -91354,\n",
       "  22902,\n",
       "  34384,\n",
       "  76444,\n",
       "  116291,\n",
       "  50447,\n",
       "  12559,\n",
       "  169752,\n",
       "  161524,\n",
       "  54731,\n",
       "  10340,\n",
       "  49285,\n",
       "  113242,\n",
       "  124874,\n",
       "  150486,\n",
       "  31717,\n",
       "  112703,\n",
       "  77466,\n",
       "  159415,\n",
       "  21951,\n",
       "  -75312,\n",
       "  -96453,\n",
       "  -110420,\n",
       "  -48794,\n",
       "  -30556,\n",
       "  -37179,\n",
       "  -21632,\n",
       "  14138,\n",
       "  -13531,\n",
       "  128825,\n",
       "  41032,\n",
       "  80295,\n",
       "  129648,\n",
       "  95244,\n",
       "  140014,\n",
       "  116153,\n",
       "  140945,\n",
       "  124711,\n",
       "  11560,\n",
       "  62032,\n",
       "  72888,\n",
       "  170931,\n",
       "  181702,\n",
       "  122301,\n",
       "  114678,\n",
       "  52640,\n",
       "  -49761,\n",
       "  128694,\n",
       "  -152108,\n",
       "  -163209,\n",
       "  -30101,\n",
       "  -19784,\n",
       "  24600,\n",
       "  -26575,\n",
       "  25520,\n",
       "  -12882,\n",
       "  31818,\n",
       "  59534,\n",
       "  -42846,\n",
       "  129393,\n",
       "  220133,\n",
       "  50287,\n",
       "  53522,\n",
       "  121448,\n",
       "  228829,\n",
       "  118778,\n",
       "  5579,\n",
       "  37812,\n",
       "  191987,\n",
       "  126633,\n",
       "  235834,\n",
       "  -72411,\n",
       "  64980,\n",
       "  -3715,\n",
       "  -2407,\n",
       "  125055,\n",
       "  -121835,\n",
       "  -141301,\n",
       "  -24658,\n",
       "  -1960,\n",
       "  23917,\n",
       "  -6461,\n",
       "  -14932,\n",
       "  -39107,\n",
       "  -44386,\n",
       "  -65243,\n",
       "  -19334,\n",
       "  204054,\n",
       "  219716,\n",
       "  128314,\n",
       "  -40698,\n",
       "  101428,\n",
       "  192986,\n",
       "  -22354,\n",
       "  52807,\n",
       "  36915,\n",
       "  230646,\n",
       "  220883,\n",
       "  120646,\n",
       "  -11411,\n",
       "  -5242,\n",
       "  81465,\n",
       "  -10230,\n",
       "  15826,\n",
       "  -71295,\n",
       "  -72844,\n",
       "  -17530,\n",
       "  -37495,\n",
       "  -4727,\n",
       "  -35484,\n",
       "  7911,\n",
       "  -35284,\n",
       "  -64050,\n",
       "  -61408,\n",
       "  -58452,\n",
       "  -10313,\n",
       "  43096,\n",
       "  206953,\n",
       "  23874,\n",
       "  128265,\n",
       "  123335,\n",
       "  -38500,\n",
       "  97430,\n",
       "  132273,\n",
       "  220711,\n",
       "  184076,\n",
       "  119327,\n",
       "  24586,\n",
       "  -31352,\n",
       "  98465,\n",
       "  144711,\n",
       "  1220,\n",
       "  -14239,\n",
       "  -121963,\n",
       "  -35021,\n",
       "  -16032,\n",
       "  -32241,\n",
       "  33499,\n",
       "  -38667,\n",
       "  8638,\n",
       "  -51168,\n",
       "  -4514,\n",
       "  -97273,\n",
       "  -77225,\n",
       "  1299,\n",
       "  47211,\n",
       "  -11099,\n",
       "  37734,\n",
       "  90952,\n",
       "  46571,\n",
       "  54326,\n",
       "  180587,\n",
       "  86845,\n",
       "  231881,\n",
       "  137669,\n",
       "  118150,\n",
       "  231849,\n",
       "  49648,\n",
       "  139952,\n",
       "  -45712,\n",
       "  -72440,\n",
       "  -43583,\n",
       "  -50722,\n",
       "  -29586,\n",
       "  -22485,\n",
       "  -28799,\n",
       "  7632,\n",
       "  -20076,\n",
       "  -15819,\n",
       "  21231,\n",
       "  44625,\n",
       "  32290,\n",
       "  -33332,\n",
       "  -91501,\n",
       "  -27786,\n",
       "  -12680,\n",
       "  150386,\n",
       "  -59011,\n",
       "  -83846,\n",
       "  21483,\n",
       "  -7768,\n",
       "  54645,\n",
       "  113192,\n",
       "  59750,\n",
       "  130125,\n",
       "  12852,\n",
       "  122167,\n",
       "  -71787,\n",
       "  -42965,\n",
       "  -25828,\n",
       "  -41188,\n",
       "  8725,\n",
       "  -34150,\n",
       "  30766,\n",
       "  19988,\n",
       "  -14560,\n",
       "  -9416,\n",
       "  9443,\n",
       "  8857,\n",
       "  26938,\n",
       "  30283,\n",
       "  -140283,\n",
       "  -159569,\n",
       "  -141789,\n",
       "  -94761,\n",
       "  -93037,\n",
       "  -47344,\n",
       "  -40354,\n",
       "  33730,\n",
       "  37987,\n",
       "  31911,\n",
       "  84098,\n",
       "  -4831,\n",
       "  -51482,\n",
       "  -33076,\n",
       "  -76531,\n",
       "  -56853,\n",
       "  -15512,\n",
       "  -16462,\n",
       "  25553,\n",
       "  25276,\n",
       "  -13945,\n",
       "  -26257,\n",
       "  24638,\n",
       "  7813,\n",
       "  -38408,\n",
       "  -3152,\n",
       "  -79104,\n",
       "  -142767,\n",
       "  -116295,\n",
       "  -74665,\n",
       "  -61763,\n",
       "  -96806,\n",
       "  92989,\n",
       "  27456,\n",
       "  -61074,\n",
       "  -109481,\n",
       "  -115419,\n",
       "  -32642,\n",
       "  -121046,\n",
       "  -125295,\n",
       "  -182501,\n",
       "  -119968,\n",
       "  -48428,\n",
       "  -65516,\n",
       "  -66454,\n",
       "  -61478,\n",
       "  -21655,\n",
       "  -31623,\n",
       "  14895,\n",
       "  -6362,\n",
       "  10804,\n",
       "  -47589,\n",
       "  -46604,\n",
       "  387,\n",
       "  -124912,\n",
       "  -236323,\n",
       "  -50531,\n",
       "  -136676,\n",
       "  -92305,\n",
       "  -205287,\n",
       "  -191634,\n",
       "  -163640,\n",
       "  -261241,\n",
       "  -251479,\n",
       "  -134009,\n",
       "  -275224,\n",
       "  -277793,\n",
       "  -128364,\n",
       "  -208337,\n",
       "  -61885,\n",
       "  -21298,\n",
       "  -87319,\n",
       "  -41805,\n",
       "  -58488,\n",
       "  5734,\n",
       "  29788,\n",
       "  -19849,\n",
       "  -14085,\n",
       "  29475,\n",
       "  6057,\n",
       "  -72633,\n",
       "  -95594,\n",
       "  -87116,\n",
       "  -88835,\n",
       "  -44082,\n",
       "  -126256,\n",
       "  -214308,\n",
       "  -181334,\n",
       "  -229783,\n",
       "  -234991,\n",
       "  -202191,\n",
       "  -226128,\n",
       "  -275817,\n",
       "  -106686,\n",
       "  -101254,\n",
       "  -95762,\n",
       "  -51142,\n",
       "  -18819,\n",
       "  -41415,\n",
       "  -5067,\n",
       "  -15004,\n",
       "  -33497,\n",
       "  -38932,\n",
       "  -18279,\n",
       "  18796,\n",
       "  31044,\n",
       "  -30560,\n",
       "  15218,\n",
       "  -25283,\n",
       "  -48746,\n",
       "  -51491,\n",
       "  18336,\n",
       "  40820,\n",
       "  52736,\n",
       "  42367,\n",
       "  -10988,\n",
       "  -46559,\n",
       "  -49559,\n",
       "  -125334,\n",
       "  -75599,\n",
       "  4423,\n",
       "  76886,\n",
       "  -40357,\n",
       "  82960,\n",
       "  134805,\n",
       "  42471,\n",
       "  12407,\n",
       "  -31880,\n",
       "  15538,\n",
       "  -35351,\n",
       "  -273,\n",
       "  13808,\n",
       "  -22270,\n",
       "  7121,\n",
       "  -15028,\n",
       "  -12487,\n",
       "  -21158,\n",
       "  96634,\n",
       "  76932,\n",
       "  116863,\n",
       "  85092,\n",
       "  29180,\n",
       "  87150,\n",
       "  70552,\n",
       "  36816,\n",
       "  63448,\n",
       "  99034,\n",
       "  52621,\n",
       "  81499,\n",
       "  -28582,\n",
       "  104648,\n",
       "  114758,\n",
       "  155725,\n",
       "  69693,\n",
       "  -58559,\n",
       "  6174,\n",
       "  -34706,\n",
       "  -1036,\n",
       "  2777,\n",
       "  -33257,\n",
       "  -15489,\n",
       "  -8817,\n",
       "  -4946,\n",
       "  -32555,\n",
       "  -19346,\n",
       "  23667,\n",
       "  110511,\n",
       "  76972,\n",
       "  60790,\n",
       "  112196,\n",
       "  98745,\n",
       "  105270,\n",
       "  98852,\n",
       "  92508,\n",
       "  127745,\n",
       "  29467,\n",
       "  56287,\n",
       "  59558,\n",
       "  59686,\n",
       "  90489,\n",
       "  34539,\n",
       "  -30192,\n",
       "  -2264,\n",
       "  39582,\n",
       "  -1158,\n",
       "  -28605,\n",
       "  -29447,\n",
       "  -35311,\n",
       "  35659,\n",
       "  -33159,\n",
       "  -24828,\n",
       "  -6988,\n",
       "  6298,\n",
       "  16327,\n",
       "  -1352,\n",
       "  10487,\n",
       "  -18581,\n",
       "  1064,\n",
       "  15699,\n",
       "  5677,\n",
       "  -14686,\n",
       "  20322,\n",
       "  57411,\n",
       "  36201,\n",
       "  11688,\n",
       "  13739,\n",
       "  46334,\n",
       "  -5919,\n",
       "  19092,\n",
       "  -4797,\n",
       "  -27025,\n",
       "  -15252,\n",
       "  23404,\n",
       "  -7730,\n",
       "  -19525,\n",
       "  17676,\n",
       "  3975,\n",
       "  -14711,\n",
       "  -10779,\n",
       "  21910,\n",
       "  -13818,\n",
       "  -2108,\n",
       "  -27081,\n",
       "  -24261,\n",
       "  25376,\n",
       "  32145,\n",
       "  -7014,\n",
       "  642,\n",
       "  7164,\n",
       "  -18064,\n",
       "  27919,\n",
       "  22698,\n",
       "  -2587,\n",
       "  13026,\n",
       "  -26069,\n",
       "  -24787,\n",
       "  17768,\n",
       "  29369,\n",
       "  -1962,\n",
       "  -20892,\n",
       "  24218,\n",
       "  -11458,\n",
       "  -30427,\n",
       "  15752,\n",
       "  2720,\n",
       "  -13965,\n",
       "  33821,\n",
       "  -11229,\n",
       "  6887,\n",
       "  -29590,\n",
       "  33151,\n",
       "  -13858,\n",
       "  14921,\n",
       "  -2299,\n",
       "  21223,\n",
       "  27596,\n",
       "  3375,\n",
       "  8287,\n",
       "  3343,\n",
       "  28646,\n",
       "  70757,\n",
       "  53897,\n",
       "  8048,\n",
       "  -3757,\n",
       "  -31981,\n",
       "  2599,\n",
       "  -5524,\n",
       "  -30000,\n",
       "  -34798,\n",
       "  5237,\n",
       "  7947,\n",
       "  18256,\n",
       "  -13466,\n",
       "  9500,\n",
       "  4922,\n",
       "  -20100,\n",
       "  7556,\n",
       "  -1579,\n",
       "  13437,\n",
       "  16289,\n",
       "  -36062,\n",
       "  -39564,\n",
       "  -58792,\n",
       "  -63188,\n",
       "  -76757,\n",
       "  -61583,\n",
       "  -11200,\n",
       "  52348,\n",
       "  107316,\n",
       "  49832,\n",
       "  -40026,\n",
       "  -62359,\n",
       "  -66780,\n",
       "  -49624,\n",
       "  -71837,\n",
       "  -559,\n",
       "  -27057,\n",
       "  -31899,\n",
       "  -8570,\n",
       "  24620,\n",
       "  30277,\n",
       "  14732,\n",
       "  -4438,\n",
       "  -13898,\n",
       "  -19812,\n",
       "  15088,\n",
       "  9278,\n",
       "  9539,\n",
       "  20844,\n",
       "  55951,\n",
       "  82057,\n",
       "  -17076,\n",
       "  24459,\n",
       "  3891,\n",
       "  -122311,\n",
       "  -52201,\n",
       "  -21054,\n",
       "  -13551,\n",
       "  -61510,\n",
       "  -16181,\n",
       "  -49654,\n",
       "  -98143,\n",
       "  -126535,\n",
       "  -83591,\n",
       "  -56666,\n",
       "  -4879,\n",
       "  22322,\n",
       "  9251,\n",
       "  -30068,\n",
       "  -27686,\n",
       "  -2496,\n",
       "  9216,\n",
       "  6254,\n",
       "  42881,\n",
       "  30563,\n",
       "  105557,\n",
       "  137458,\n",
       "  151732,\n",
       "  264171,\n",
       "  221624,\n",
       "  160383,\n",
       "  102657,\n",
       "  161297,\n",
       "  92151,\n",
       "  38888,\n",
       "  -8202,\n",
       "  -47146,\n",
       "  -120151,\n",
       "  -153102,\n",
       "  -142560,\n",
       "  -144246,\n",
       "  -182308,\n",
       "  -99388,\n",
       "  -48976,\n",
       "  35944,\n",
       "  8782,\n",
       "  8042,\n",
       "  2878,\n",
       "  -32464,\n",
       "  -22538,\n",
       "  17170,\n",
       "  108938,\n",
       "  165972,\n",
       "  205121,\n",
       "  113816,\n",
       "  91680,\n",
       "  124310,\n",
       "  32908,\n",
       "  60401,\n",
       "  93624,\n",
       "  82919,\n",
       "  47045,\n",
       "  87475,\n",
       "  130300,\n",
       "  107264,\n",
       "  75700,\n",
       "  69833,\n",
       "  111806,\n",
       "  34061,\n",
       "  -38060,\n",
       "  -49078,\n",
       "  44534,\n",
       "  -7447,\n",
       "  808,\n",
       "  24230,\n",
       "  12219,\n",
       "  -11523,\n",
       "  15652,\n",
       "  -17480,\n",
       "  56061,\n",
       "  131979,\n",
       "  -37991,\n",
       "  -7837,\n",
       "  -5270,\n",
       "  54239,\n",
       "  71156,\n",
       "  -5264,\n",
       "  26616,\n",
       "  99939,\n",
       "  67889,\n",
       "  -34031,\n",
       "  -77443,\n",
       "  76607,\n",
       "  151,\n",
       "  -14627,\n",
       "  -33834,\n",
       "  -53107,\n",
       "  -16226,\n",
       "  237349,\n",
       "  179933,\n",
       "  8177,\n",
       "  -57219,\n",
       "  -12276,\n",
       "  26455,\n",
       "  -10740,\n",
       "  -78524,\n",
       "  20724,\n",
       "  33222,\n",
       "  -28196,\n",
       "  -82647,\n",
       "  108120,\n",
       "  -32217,\n",
       "  -55425,\n",
       "  -105984,\n",
       "  -15411,\n",
       "  -269,\n",
       "  -8001,\n",
       "  2383,\n",
       "  -7242,\n",
       "  -50987,\n",
       "  13501,\n",
       "  -26659,\n",
       "  ...],\n",
       " 'hid_bias': [119241721928,\n",
       "  -257020682096,\n",
       "  -365726113319,\n",
       "  -537121474743,\n",
       "  -492578864098,\n",
       "  -612299919128,\n",
       "  -614297628403,\n",
       "  1199751257896,\n",
       "  151110604405,\n",
       "  51110852510,\n",
       "  -191845238209,\n",
       "  242919623852,\n",
       "  -1057520270348,\n",
       "  -378774136305,\n",
       "  108412228525,\n",
       "  218202084303,\n",
       "  998954296112,\n",
       "  539191246033,\n",
       "  378161668777,\n",
       "  169919028878],\n",
       " 'out_weights': [-410390,\n",
       "  -886455,\n",
       "  -953463,\n",
       "  -1193870,\n",
       "  -964690,\n",
       "  656051,\n",
       "  -1438019,\n",
       "  735168,\n",
       "  18905,\n",
       "  666971,\n",
       "  505006,\n",
       "  -870126,\n",
       "  734439,\n",
       "  -799626,\n",
       "  617670,\n",
       "  631743,\n",
       "  813552,\n",
       "  -1137027,\n",
       "  -323673,\n",
       "  -1074705,\n",
       "  -1294772,\n",
       "  641955,\n",
       "  -756081,\n",
       "  -640766,\n",
       "  829333,\n",
       "  -923420,\n",
       "  711441,\n",
       "  -1243064,\n",
       "  -506436,\n",
       "  -847489,\n",
       "  -321512,\n",
       "  -999014,\n",
       "  -1173742,\n",
       "  739065,\n",
       "  -1296797,\n",
       "  -586276,\n",
       "  578376,\n",
       "  1063613,\n",
       "  -783714,\n",
       "  839803,\n",
       "  694608,\n",
       "  795757,\n",
       "  -1001396,\n",
       "  430805,\n",
       "  903613,\n",
       "  498368,\n",
       "  -253683,\n",
       "  -268133,\n",
       "  1282553,\n",
       "  90977,\n",
       "  517873,\n",
       "  -232601,\n",
       "  650644,\n",
       "  766509,\n",
       "  102834,\n",
       "  585154,\n",
       "  826140,\n",
       "  -111728,\n",
       "  749218,\n",
       "  -156297,\n",
       "  -281023,\n",
       "  772366,\n",
       "  1521384,\n",
       "  191694,\n",
       "  522087,\n",
       "  622204,\n",
       "  440992,\n",
       "  -781895,\n",
       "  1193470,\n",
       "  -427322,\n",
       "  470992,\n",
       "  -218789,\n",
       "  289565,\n",
       "  225757,\n",
       "  -373517,\n",
       "  580060,\n",
       "  -28472,\n",
       "  1379522,\n",
       "  163535,\n",
       "  -736452,\n",
       "  704090,\n",
       "  -1132047,\n",
       "  -3106,\n",
       "  596179,\n",
       "  -1111366,\n",
       "  -1176782,\n",
       "  653372,\n",
       "  585326,\n",
       "  -1003267,\n",
       "  260547,\n",
       "  -704025,\n",
       "  543228,\n",
       "  -915727,\n",
       "  -396484,\n",
       "  587841,\n",
       "  -1424094,\n",
       "  -77465,\n",
       "  -381106,\n",
       "  696567,\n",
       "  -518465,\n",
       "  -13608,\n",
       "  602867,\n",
       "  1513815,\n",
       "  366702,\n",
       "  -220042,\n",
       "  633111,\n",
       "  -312875,\n",
       "  527967,\n",
       "  -80699,\n",
       "  550423,\n",
       "  443070,\n",
       "  501710,\n",
       "  104178,\n",
       "  176844,\n",
       "  613805,\n",
       "  507168,\n",
       "  261830,\n",
       "  1342869,\n",
       "  372963,\n",
       "  874567,\n",
       "  729839,\n",
       "  -465897,\n",
       "  -166324,\n",
       "  -166142,\n",
       "  -885789,\n",
       "  -255670,\n",
       "  -1058263,\n",
       "  -235567,\n",
       "  -1126365,\n",
       "  583124,\n",
       "  515427,\n",
       "  -1250427,\n",
       "  -38326,\n",
       "  753994,\n",
       "  655042,\n",
       "  -1131182,\n",
       "  841032,\n",
       "  -1007418,\n",
       "  744571,\n",
       "  904765,\n",
       "  -209164,\n",
       "  -688428,\n",
       "  -790557,\n",
       "  -817431,\n",
       "  898699,\n",
       "  -752804,\n",
       "  -103183,\n",
       "  726796,\n",
       "  1110206,\n",
       "  -1000122,\n",
       "  -1444042,\n",
       "  593790,\n",
       "  -199465,\n",
       "  -119041,\n",
       "  -1156105,\n",
       "  627657,\n",
       "  -1159955,\n",
       "  -938825,\n",
       "  -799187,\n",
       "  -1062964,\n",
       "  -447840,\n",
       "  654701,\n",
       "  83920,\n",
       "  548839,\n",
       "  820861,\n",
       "  581445,\n",
       "  568177,\n",
       "  -579576,\n",
       "  -17124,\n",
       "  505145,\n",
       "  411841,\n",
       "  528504,\n",
       "  565050,\n",
       "  659370,\n",
       "  505988,\n",
       "  380018,\n",
       "  -368938,\n",
       "  -341736,\n",
       "  -393395,\n",
       "  596826,\n",
       "  627835,\n",
       "  -810078,\n",
       "  1216868,\n",
       "  499471,\n",
       "  -213872,\n",
       "  63148,\n",
       "  673013,\n",
       "  -314554,\n",
       "  -831007,\n",
       "  -607500,\n",
       "  -709673,\n",
       "  573294,\n",
       "  455434,\n",
       "  -1292032,\n",
       "  -346530,\n",
       "  427449,\n",
       "  -1150683,\n",
       "  369863,\n",
       "  -848858,\n",
       "  -435530],\n",
       " 'out_bias': [-512028455734252928,\n",
       "  1258636355400085504,\n",
       "  -1769820690155029248,\n",
       "  -455340862274169920,\n",
       "  588846862316131584,\n",
       "  -1144255757331848192,\n",
       "  -402987152338027968,\n",
       "  1789341330528259328,\n",
       "  -631388843059539840,\n",
       "  1350203037261963008]}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ab9900fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.00918684e+19, -7.91323185e+18,  1.12167397e+19,  7.65257406e+18,\n",
       "       -1.17243376e+19,  1.17416821e+19, -7.77035761e+18, -1.24359570e+19,\n",
       "        1.86579208e+19, -3.24000025e+18])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y[1][0].numpy()*(10**18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b445be26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vx/mp1crwqx5yg2cjnczjljpjmh0000gn/T/ipykernel_47473/866016.py:3: RuntimeWarning: invalid value encountered in cast\n",
      "  \"out\": (y[1][0].detach().numpy()*(10**18)).round().astype(int).tolist(),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'scale': 1e-18,\n",
       " 'out': [-9223372036854775808,\n",
       "  -7913231849670410240,\n",
       "  9223372036854775807,\n",
       "  7652574062347412480,\n",
       "  -9223372036854775808,\n",
       "  9223372036854775807,\n",
       "  -7770357608795165696,\n",
       "  -9223372036854775808,\n",
       "  9223372036854775807,\n",
       "  -3240000247955322368],\n",
       " 'label': 8}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_json = {\n",
    "    \"scale\": 10**-18,\n",
    "    \"out\": (y[1][0].detach().numpy()*(10**18)).round().astype(int).tolist(),\n",
    "    \"label\": int(y[1].argmax())\n",
    "}\n",
    "out_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1b05bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"mnist_fc_input.json\", \"w\") as f:\n",
    "    json.dump(in_json, f)\n",
    "with open(\"mnist_fc_output.json\", \"w\") as f:\n",
    "    json.dump(out_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "548fc104",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mnist_fc_test.txt', 'w') as f:\n",
    "    for key in in_json:\n",
    "        f.write(key + \" = [\")\n",
    "        for idx, value in enumerate(in_json[key]):\n",
    "            if(idx!=0):\n",
    "                f.write(', ')\n",
    "            f.write('\"' + str(value) + '\"')\n",
    "        f.write(']\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9166f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "(y[2].numpy()*10**12).round().astype(int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7d3dcbc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_test*(10**6)).round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3e4043c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = net.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ceca185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "model_name = 'mnist_fc'\n",
    "\n",
    "with open(f\"tests_{model_name}.txt\", 'w') as f:\n",
    "    for i in range(len(X_test[:n_samples])):\n",
    "        f.write('#[test]\\n')\n",
    "        f.write(f\"fn test_{model_name}_{i+1:03d}\" + \"() {\\n\")\n",
    "        f.write(\"  let sample = [\")\n",
    "        for idx, value in enumerate(X_test[i]):\n",
    "            if(idx!=0):\n",
    "                f.write(', ')\n",
    "            f.write(str((value*(10**6)).round().astype(int)))\n",
    "        f.write('];\\n')\n",
    "        f.write(f\"  assert({model_name}(sample) == \" + str(predictions[i]) + \");\\n\")\n",
    "        f.write('}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c3b4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
