{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c22ba79",
   "metadata": {},
   "source": [
    "# Multi-Class Classification with Neural Networks\n",
    "\n",
    "In this notebook, we will look at the multi-class classification problem using a simple feedforward neural network. We will use the MNIST dataset consisting of handwritten digit images, and leverage PyTorch and Skorch for defining and training our classification model. We will save the weights of the trained model in a json file which will be used in our zk program. We will also save some samples and their corresponding label predictions from our trained model in order to verify the correctness of our zk program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b95d05e",
   "metadata": {},
   "source": [
    "## 1. Importing Necessary Libraries\n",
    "\n",
    "Here, we import the libraries and packages necessary for data generation, manipulation, visualization, neural network modeling and training, and saving the model weights and test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364c3303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b451c3ba",
   "metadata": {},
   "source": [
    "## 2. Setting Random Seeds\n",
    "\n",
    "To ensure reproducibility and get consistent results, we set random seeds for all potential sources of randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364d51c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58023626",
   "metadata": {},
   "source": [
    "## 3. Data Downloading and Preparation\n",
    "\n",
    "For our classification task, we will download the MNIST dataset (https://en.wikipedia.org/wiki/MNIST_database). The dataset has 10 classes, corresponding to the digits 0-9. We prepare the data for our training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5053da8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mnist_fc\"\n",
    "\n",
    "# Download the dataset\n",
    "mnist = fetch_openml('mnist_784', as_frame=False, cache=False, parser='auto')\n",
    "\n",
    "# Prepare the data\n",
    "X, y = mnist.data.astype(np.float32), mnist.target.astype(np.int64)\n",
    "X /= 255.0\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90f1f75",
   "metadata": {},
   "source": [
    "## 4. Data Visualization\n",
    "\n",
    "We visualize a few samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1371b61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "def plot_example(X, y, n_samples):\n",
    "    n_cols = 8\n",
    "    n_rows = math.ceil(n_samples / n_cols)\n",
    "    plt.figure(figsize=(1.5*n_cols, 2*n_rows))\n",
    "\n",
    "    for i, (img, y) in enumerate(zip(X[:n_samples].reshape(n_samples, 28, 28), y[:n_samples])):\n",
    "        plt.subplot(n_rows, n_cols, i+1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title(y, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_name}_visualization.png\", format='png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "        \n",
    "plot_example(X_train, y_train, n_samples=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceec2ca",
   "metadata": {},
   "source": [
    "## 5. Neural Network Architecture\n",
    "\n",
    "Our neural network is a fully-connected network which comprises an input layer, one hidden layer with relu activation function, and an output layer. Additionally, dropout is included for regularization. We also define the number of epochs and learning rate for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df4f1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "hidden_dims = [20]\n",
    "output_dim = 10\n",
    "\n",
    "class ClassifierModule(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim=input_dim,\n",
    "            hidden_dims=hidden_dims,\n",
    "            output_dim=output_dim,\n",
    "            nonlin=F.relu,\n",
    "            dropout=0.5,\n",
    "    ):\n",
    "        super(ClassifierModule, self).__init__()\n",
    "        self.nonlin = nonlin\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(dims) - 1):\n",
    "            self.layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            X = layer(X)\n",
    "            if i != len(self.layers) - 1:  # If not the last layer\n",
    "                X = self.nonlin(X)\n",
    "                X = self.dropout(X)\n",
    "            else:  # If last layer\n",
    "                X = F.softmax(X, dim=-1)\n",
    "        return X\n",
    "    \n",
    "net = NeuralNetClassifier(\n",
    "    ClassifierModule,\n",
    "    max_epochs=50,\n",
    "    lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857fbf46",
   "metadata": {},
   "source": [
    "## 6. Model Training\n",
    "\n",
    "We now proceed to train our neural network model using the training data. Training involves feeding our network the training data multiple times (epochs) and adjusting the model weights to minimize the classification error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ec05fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce7551e",
   "metadata": {},
   "source": [
    "## 7. Model Testing\n",
    "\n",
    "We then evaluate our model's performance on unseen data (test data). This gives us an indication of whether our model has truly learned the underlying patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33acc6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = net.predict(X_test)\n",
    "\n",
    "accuracy = (sum(y_test == y_pred) / len(y_test)) * 100\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530c9fa0",
   "metadata": {},
   "source": [
    "## 8. Save Model Parameters\n",
    "\n",
    "A zk program usually does not support `floats`. Therefore, we define a base_scaling_factor, which is used to scale the inputs and the weights and convert them to integers. Since bias is added to `(input * weight)`, it has a scaling factor at the first layer which is twice of the base_scaling_factor. The scaling factor for biases in subsequent layers is adjusted accordingly.\n",
    "\n",
    "We will save the trained model parameters (weights and biases) in a JSON file, making them ready for integration into a zk program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736af30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_scaling_factor = 10**6\n",
    "\n",
    "def get_scaling_factor(layer_idx):\n",
    "    scaling_factors = {\n",
    "        \"weight\": base_scaling_factor,\n",
    "        \"bias\": base_scaling_factor ** (layer_idx + 1)\n",
    "    }\n",
    "    return scaling_factors\n",
    "\n",
    "model_data = {}\n",
    "\n",
    "for param in net.get_all_learnable_params():\n",
    "    model_data[param[0]] = param[1].detach().numpy().flatten()\n",
    "\n",
    "model_json = {}\n",
    "num_layers = len(model_data) // 2  # considering weight and bias for each layer\n",
    "\n",
    "for i in range(num_layers):\n",
    "    weight_key = f\"layers.{i}.weight\"\n",
    "    bias_key = f\"layers.{i}.bias\"\n",
    "    \n",
    "    scaling = get_scaling_factor(i + 1)\n",
    "        \n",
    "    model_json[f\"l{i+1}_weights\"] = (model_data[weight_key] * scaling[\"weight\"]).round().astype(int).tolist()\n",
    "    model_json[f\"l{i+1}_biases\"] = (model_data[bias_key] * scaling[\"bias\"]).round().astype(int).tolist()\n",
    "\n",
    "# Save to JSON\n",
    "with open(f\"{model_name}_parameters.json\", \"w\") as f:\n",
    "    json.dump(model_json, f, indent=4)\n",
    "\n",
    "print(f\"Saved model parameters at {model_name}_parameters.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e99feb",
   "metadata": {},
   "source": [
    "## 9. Save Test Samples\n",
    "\n",
    "We also save the test sample inputs and corresponding model predictions in a json file. They will serve as a benchmark, enabling us to test the correctness of our zk program. The samples are again scaled appropriately for use in zk programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9302891",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "\n",
    "samples_json = {}\n",
    "\n",
    "for (i, sample) in enumerate(zip(X_test[:n_samples], y_pred[:n_samples])):\n",
    "    samples_json[f\"input{i+1}\"] = (sample[0]*(base_scaling_factor)).round().astype(int).tolist()\n",
    "    samples_json[f\"output{i+1}\"] = sample[1].tolist()\n",
    "    \n",
    "# Save to JSON\n",
    "with open(f\"{model_name}_samples.json\", \"w\") as f:\n",
    "    json.dump(samples_json, f, indent=4)\n",
    "\n",
    "print(f\"Saved test samples at {model_name}_samples.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6493ce06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
